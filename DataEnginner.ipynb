{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ce64cc39-5f61-4490-ba37-d52c35ec4c82",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openpyxl in c:\\users\\doris\\anaconda3\\envs\\jlab312\\lib\\site-packages (3.1.5)\n",
      "Requirement already satisfied: et-xmlfile in c:\\users\\doris\\anaconda3\\envs\\jlab312\\lib\\site-packages (from openpyxl) (2.0.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install openpyxl\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e31b9fb3-50ef-43c9-904a-5e65b191d7e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 22068 JSON file records to C:\\Users\\doris\\Desktop\\IKT Project\\json_files.xlsx\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "directory = r\"C:\\Users\\doris\\Downloads\\dataset\"\n",
    "\n",
    "# Collect all JSON file paths\n",
    "records = []\n",
    "for root, dirs, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if file.lower().endswith(\".json\"):\n",
    "            records.append({\n",
    "                \"json_name\": file,\n",
    "                \"directory\": root\n",
    "            })\n",
    "\n",
    "# Convert to DataFrame\n",
    "df = pd.DataFrame(records)\n",
    "\n",
    "# Save to Excel on Desktop\n",
    "output_file = r\"C:\\Users\\doris\\Desktop\\IKT Project\\json_files.xlsx\"\n",
    "df.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Saved {len(df)} JSON file records to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "636fc60a-136f-44f0-8361-26e0c302bba3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                       json_name  \\\n",
      "0  guatemala-volcano_00000004_post_disaster.json   \n",
      "1   guatemala-volcano_00000004_pre_disaster.json   \n",
      "2  guatemala-volcano_00000012_post_disaster.json   \n",
      "3   guatemala-volcano_00000012_pre_disaster.json   \n",
      "4  guatemala-volcano_00000014_post_disaster.json   \n",
      "\n",
      "                                           directory  \\\n",
      "0  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\hold...   \n",
      "1  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\hold...   \n",
      "2  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\hold...   \n",
      "3  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\hold...   \n",
      "4  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\hold...   \n",
      "\n",
      "                                        filename disaster_type  items  \\\n",
      "0  guatemala-volcano_00000004_post_disaster.json       volcano   20.0   \n",
      "1                                           None          None    NaN   \n",
      "2  guatemala-volcano_00000012_post_disaster.json       volcano   33.0   \n",
      "3                                           None          None    NaN   \n",
      "4  guatemala-volcano_00000014_post_disaster.json       volcano    7.0   \n",
      "\n",
      "       area_m2   weighted_m2       folder  \n",
      "0  7918.894097  13518.558035  hold_labels  \n",
      "1          NaN           NaN         None  \n",
      "2  2229.724748      0.000000  hold_labels  \n",
      "3          NaN           NaN         None  \n",
      "4   692.157327   1471.381312  hold_labels  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load the Excel files\n",
    "dataset_info = pd.read_excel(r\"C:\\Users\\doris\\Desktop\\IKT Project\\dataset_info_post.xlsx\", sheet_name=\"Sheet1\")\n",
    "json_files = pd.read_excel(r\"C:\\Users\\doris\\Desktop\\IKT Project\\json_files.xlsx\")\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "# Write both DataFrames into the database\n",
    "dataset_info.to_sql(\"dataset_info\", conn, index=False, if_exists=\"replace\")\n",
    "json_files.to_sql(\"json_files\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "# Example SQL JOIN (adjust join column names as needed)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    jf.*,di.*\n",
    "FROM json_files jf\n",
    "LEFT JOIN dataset_info di\n",
    "on jf.json_name = di.filename\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "result = pd.read_sql(query, conn)\n",
    "\n",
    "print(result.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "83c17422-e66e-4850-a608-dc22e0530877",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Joined result saved to C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load the Excel files\n",
    "dataset_info = pd.read_excel(r\"C:\\Users\\doris\\Desktop\\IKT Project\\dataset_info_post.xlsx\", sheet_name=\"Sheet1\")\n",
    "json_files = pd.read_excel(r\"C:\\Users\\doris\\Desktop\\IKT Project\\json_files.xlsx\")\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "# Write both DataFrames into the database\n",
    "dataset_info.to_sql(\"dataset_info\", conn, index=False, if_exists=\"replace\")\n",
    "json_files.to_sql(\"json_files\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "# Example SQL JOIN (adjust join keys if needed)\n",
    "query = \"\"\"\n",
    "SELECT \n",
    "    jf.*,di.*\n",
    "FROM json_files jf\n",
    "LEFT JOIN dataset_info di\n",
    "on jf.json_name = di.filename\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "# Run query\n",
    "result = pd.read_sql(query, conn)\n",
    "\n",
    "# Save result to a new Excel file\n",
    "output_file = r\"C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\"\n",
    "result.to_excel(output_file, index=False)\n",
    "\n",
    "print(f\"Joined result saved to {output_file}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8f145ecf-2c14-4c82-80f1-555b45637a20",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'divided_value' added and file updated: C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Path to the existing file\n",
    "file_path = r\"C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "df = pd.read_excel(file_path)\n",
    "\n",
    "# Create new column safely\n",
    "df[\"divided_value\"] = df.apply(\n",
    "    lambda row: row[\"weighted_m2\"] / row[\"area_m2\"] \n",
    "    if pd.notnull(row[\"weighted_m2\"]) and pd.notnull(row[\"area_m2\"]) and row[\"area_m2\"] != 0 \n",
    "    else None, \n",
    "    axis=1\n",
    ")\n",
    "\n",
    "# Overwrite the same Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"Column 'divided_value' added and file updated: {file_path}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cb2b4191-c33d-4ba6-a2f7-dc5332a668f8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'pre_post_flag' added and file updated: C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Add new column based on json_name\n",
    "df[\"pre_post_flag\"] = df[\"json_name\"].apply(\n",
    "    lambda x: \"post\" if isinstance(x, str) and \"post\" in x.lower()\n",
    "    else (\"pre\" if isinstance(x, str) and \"pre\" in x.lower()\n",
    "    else None)\n",
    ")\n",
    "\n",
    "# Overwrite the same Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"Column 'pre_post_flag' added and file updated: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "c2819427-a034-41e1-8f36-0acbc4d7cff6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Column 'base_id' added and file updated: C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\n"
     ]
    }
   ],
   "source": [
    "# Extract base id (remove _pre_disaster.json / _post_disaster.json)\n",
    "df[\"base_id\"] = df[\"json_name\"].str.replace(r\"_(pre|post)_disaster\\.json$\", \"\", regex=True)\n",
    "\n",
    "# Overwrite the same Excel file\n",
    "df.to_excel(file_path, index=False)\n",
    "\n",
    "print(f\"Column 'base_id' added and file updated: {file_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "fa48e9fd-5546-4165-ade4-70cb11934a40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in DB:\n",
      "       name\n",
      "0  prepost\n",
      "  disaster_type  floor_int                                      json_name  \\\n",
      "0    earthquake          0  mexico-earthquake_00000181_post_disaster.json   \n",
      "1    earthquake          0  mexico-earthquake_00000076_post_disaster.json   \n",
      "2    earthquake          0  mexico-earthquake_00000053_post_disaster.json   \n",
      "3    earthquake          0  mexico-earthquake_00000095_post_disaster.json   \n",
      "4    earthquake          0  mexico-earthquake_00000059_post_disaster.json   \n",
      "\n",
      "                                           directory  \\\n",
      "0  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\tier...   \n",
      "1  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\test...   \n",
      "2  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\tier...   \n",
      "3  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\test...   \n",
      "4  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\tier...   \n",
      "\n",
      "                                        filename disaster_type:1  items  \\\n",
      "0  mexico-earthquake_00000181_post_disaster.json      earthquake  368.0   \n",
      "1  mexico-earthquake_00000076_post_disaster.json      earthquake  295.0   \n",
      "2  mexico-earthquake_00000053_post_disaster.json      earthquake  412.0   \n",
      "3  mexico-earthquake_00000095_post_disaster.json      earthquake   41.0   \n",
      "4  mexico-earthquake_00000059_post_disaster.json      earthquake  353.0   \n",
      "\n",
      "        area_m2  weighted_m2        folder  divided_value pre_post_flag  \\\n",
      "0  74679.696674  9823.717406  tier1_labels       0.131545          post   \n",
      "1  71196.052062  6144.312157   test_labels       0.086301          post   \n",
      "2  61225.297909  4510.078580  tier1_labels       0.073664          post   \n",
      "3  35936.143550  2598.731934   test_labels       0.072315          post   \n",
      "4  77609.779693  5308.624411  tier1_labels       0.068401          post   \n",
      "\n",
      "                      base_id  rn  \n",
      "0  mexico-earthquake_00000181   1  \n",
      "1  mexico-earthquake_00000076   2  \n",
      "2  mexico-earthquake_00000053   3  \n",
      "3  mexico-earthquake_00000095   4  \n",
      "4  mexico-earthquake_00000059   5  \n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# Load the Excel file\n",
    "prepost = pd.read_excel(r\"C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\", sheet_name=\"Sheet1\")\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "# Save the DataFrame to SQLite with a simple table name\n",
    "prepost.to_sql(\"prepost\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "# Optional: check available tables\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "print(\"Tables in DB:\\n\", tables)\n",
    "\n",
    "# Query the table\n",
    "query = \"\"\"\n",
    "WITH ranked AS (\n",
    "    SELECT\n",
    "        disaster_type,\n",
    "        CAST(divided_value AS INT) AS floor_int,\n",
    "        t.*,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY disaster_type, CAST(divided_value AS INT)\n",
    "            ORDER BY divided_value DESC\n",
    "        ) AS rn\n",
    "    FROM prepost t\n",
    "    where divided_value <> 0 and divided_value is not null\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked\n",
    "WHERE rn <= 1000\n",
    "ORDER BY disaster_type, floor_int, rn;\n",
    "\n",
    "\"\"\"\n",
    "result = pd.read_sql(query, conn)\n",
    "\n",
    "# Show first rows\n",
    "print(result.head())\n",
    "\n",
    "# Close the connection when done\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "9a8ba1cc-353e-476f-8cd8-68542e687a21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tables in DB:\n",
      "       name\n",
      "0  prepost\n",
      "  disaster_type  floor_int                                      json_name  \\\n",
      "0    earthquake          0  mexico-earthquake_00000181_post_disaster.json   \n",
      "1    earthquake          0  mexico-earthquake_00000076_post_disaster.json   \n",
      "2    earthquake          0  mexico-earthquake_00000053_post_disaster.json   \n",
      "3    earthquake          0  mexico-earthquake_00000095_post_disaster.json   \n",
      "4    earthquake          0  mexico-earthquake_00000059_post_disaster.json   \n",
      "\n",
      "                                           directory  \\\n",
      "0  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\tier...   \n",
      "1  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\test...   \n",
      "2  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\tier...   \n",
      "3  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\test...   \n",
      "4  C:\\Users\\doris\\Downloads\\dataset\\geotiffs\\tier...   \n",
      "\n",
      "                                        filename disaster_type:1  items  \\\n",
      "0  mexico-earthquake_00000181_post_disaster.json      earthquake  368.0   \n",
      "1  mexico-earthquake_00000076_post_disaster.json      earthquake  295.0   \n",
      "2  mexico-earthquake_00000053_post_disaster.json      earthquake  412.0   \n",
      "3  mexico-earthquake_00000095_post_disaster.json      earthquake   41.0   \n",
      "4  mexico-earthquake_00000059_post_disaster.json      earthquake  353.0   \n",
      "\n",
      "        area_m2  weighted_m2        folder  divided_value pre_post_flag  \\\n",
      "0  74679.696674  9823.717406  tier1_labels       0.131545          post   \n",
      "1  71196.052062  6144.312157   test_labels       0.086301          post   \n",
      "2  61225.297909  4510.078580  tier1_labels       0.073664          post   \n",
      "3  35936.143550  2598.731934   test_labels       0.072315          post   \n",
      "4  77609.779693  5308.624411  tier1_labels       0.068401          post   \n",
      "\n",
      "                      base_id  rn  \n",
      "0  mexico-earthquake_00000181   1  \n",
      "1  mexico-earthquake_00000076   2  \n",
      "2  mexico-earthquake_00000053   3  \n",
      "3  mexico-earthquake_00000095   4  \n",
      "4  mexico-earthquake_00000059   5  \n",
      "✅ Results written to 'selected' sheet in Excel.\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import sqlite3\n",
    "\n",
    "# File path\n",
    "excel_file = r\"C:\\Users\\doris\\Desktop\\IKT Project\\Pre&Post_dataset.xlsx\"\n",
    "\n",
    "# Load the Excel file\n",
    "prepost = pd.read_excel(excel_file, sheet_name=\"Sheet1\")\n",
    "\n",
    "# Create an in-memory SQLite database\n",
    "conn = sqlite3.connect(\":memory:\")\n",
    "\n",
    "# Save the DataFrame to SQLite with a simple table name\n",
    "prepost.to_sql(\"prepost\", conn, index=False, if_exists=\"replace\")\n",
    "\n",
    "# Optional: check available tables\n",
    "tables = pd.read_sql(\"SELECT name FROM sqlite_master WHERE type='table';\", conn)\n",
    "print(\"Tables in DB:\\n\", tables)\n",
    "\n",
    "# Query the table\n",
    "query = \"\"\"\n",
    "WITH ranked AS (\n",
    "    SELECT\n",
    "        disaster_type,\n",
    "        CAST(divided_value AS INT) AS floor_int,\n",
    "        t.*,\n",
    "        ROW_NUMBER() OVER (\n",
    "            PARTITION BY disaster_type, CAST(divided_value AS INT)\n",
    "            ORDER BY divided_value DESC\n",
    "        ) AS rn\n",
    "    FROM prepost t\n",
    "    WHERE divided_value <> 0 \n",
    "      AND divided_value IS NOT NULL\n",
    ")\n",
    "SELECT *\n",
    "FROM ranked\n",
    "WHERE rn <= 1000\n",
    "ORDER BY disaster_type, floor_int, rn;\n",
    "\"\"\"\n",
    "result = pd.read_sql(query, conn)\n",
    "\n",
    "# Show first rows\n",
    "print(result.head())\n",
    "\n",
    "# Close the SQLite connection\n",
    "conn.close()\n",
    "\n",
    "# Write result back into the same Excel file, replacing \"selected\" sheet if exists\n",
    "with pd.ExcelWriter(excel_file, engine=\"openpyxl\", mode=\"a\", if_sheet_exists=\"replace\") as writer:\n",
    "    result.to_excel(writer, sheet_name=\"selected\", index=False)\n",
    "\n",
    "print(\"✅ Results written to 'selected' sheet in Excel.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fce3514e-9b78-45a6-a6bc-c5f8d777ac13",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (jlab312)",
   "language": "python",
   "name": "jlab312"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
